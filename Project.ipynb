{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pickle as pkl\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6771c6c",
   "metadata": {},
   "source": [
    "# Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/features.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(14,7))\n",
    "\n",
    "ax.plot(df['Date'], df['Return']*1000, label = 'returns')\n",
    "ax.plot(df['Date'], df['Open-Close']*1000, label = 'open-close')\n",
    "ax.plot(df['Date'], df['Open-Low']*1000, label = 'open-low')\n",
    "ax.plot(df['Date'], df['Open-High']*1000, label = 'open-high')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('â€°')\n",
    "plt.xlim(pd.to_datetime('1995-01-03'), pd.to_datetime('1995-02-05'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c246c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(14,7))\n",
    "\n",
    "ax.plot(df['Date'], df['Normalized Volume'], label = 'Normalized Volume')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Normalized Volume')\n",
    "plt.xlim(pd.to_datetime('1995-01-03'), pd.to_datetime('2000-02-05'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3e8fe",
   "metadata": {},
   "source": [
    "# Single layer MLP GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data: np.array, window_len, scaler) -> np.array:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      -  data = np.array\n",
    "      -  window_len = length of window\n",
    "      -  scaler = sklearn.preprocessing\n",
    "\n",
    "    Returns:\n",
    "      - processed: preprocessed data as python list\n",
    "    \"\"\"\n",
    "    # normalize data\n",
    "    scaler = scaler.fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    \n",
    "    # group data into windows of length window_len\n",
    "    windows = []\n",
    "    for i in range(len(data) - window_len):\n",
    "        windows.append(scaled_data[i:i+window_len])\n",
    "        \n",
    "    # reorder the data\n",
    "    idx = np.random.permutation(len(windows))\n",
    "\n",
    "    processed = []\n",
    "    for i in range(len(windows)):\n",
    "        processed.append(windows[idx[i]])\n",
    "    \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58045123",
   "metadata": {},
   "source": [
    "Very simple GAN implementation for our baseline. Will use convolutions and TimeGAN as the next models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbe6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(true_data, training_steps, generator, discriminator,lr = 0.001):\n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    # binary cross entropy loss\n",
    "    loss = nn.BCELoss()\n",
    "    true_data = torch.tensor(true_data).float()\n",
    "    true_labels = torch.tensor(np.ones((len(true_data),1))).float()\n",
    "\n",
    "    gen_data_loss = []\n",
    "    true_data_loss = []\n",
    "    for i in range(training_steps):\n",
    "        \n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # if minmaxscaling was used, the inputs should be bertween 0 and 1\n",
    "        noise = torch.rand(true_data.shape[0], true_data.shape[1])\n",
    "        \n",
    "        # generator\n",
    "        generated_data = generator(noise)\n",
    "       \n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        \n",
    "       # print(generator_discriminator_out.shape, true_labels.shape)\n",
    "        \n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        \n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)        \n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros((len(true_data),1)))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        gen_data_loss.append(generator_discriminator_loss)\n",
    "        true_data_loss.append(true_discriminator_loss)\n",
    "        discriminator_loss.backward()\n",
    "       \n",
    "        discriminator_optimizer.step()\n",
    "        if i%30==0:\n",
    "           \n",
    "            print(\"epoch: \", i, \"discriminator loss: \",  discriminator_loss.item())\n",
    "    return generator, discriminator, gen_data_loss, true_data_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95479c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minmax scaler\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "df = pd.read_csv(\"./data/features.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# set index to date\n",
    "try:\n",
    "    df = df.set_index('Date').sort_index()\n",
    "except:\n",
    "    df = df\n",
    "\n",
    "# prep data\n",
    "df = df.drop(['Close-MA 20D', 'RSI 14D','Realized Volatility 30D','VIX Move'], axis=1)\n",
    "data = prep_data(df.values, 30, scaler)\n",
    "\n",
    "# check size\n",
    "print(len(data), data[0].shape)\n",
    "data = np.array(data)\n",
    "reshaped_data =  data.reshape(len(data),data.shape[1]*data.shape[2])\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075eb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(input_length,500),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(500,1200),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(1200,800),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(800,input_length)\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448cbdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(input_length,500),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(500,400),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(400,400),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Linear(400,1),\n",
    "                                 nn.Sigmoid()\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator2 = Generator2(reshaped_data.shape[1])\n",
    "discriminator2 = Discriminator2(reshaped_data.shape[1])\n",
    "gen2, disc2, gen_loss2, disc_loss2 = train_gan(reshaped_data, 500, generator2, discriminator2,0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_loss2)\n",
    "plt.plot(disc_loss2)\n",
    "plt.legend([\"Generated Data\", \"True Data\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Binary Cross-Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.rand(reshaped_data.shape[0], reshaped_data.shape[1])\n",
    "generated_data2 = gen2(noise)\n",
    "generated_data2 = generated_data2.reshape(data.shape[0],data.shape[1],data.shape[2])\n",
    "generated_data2 = generated_data2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generated_data2[0,:,0],label='generated data for GAN2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed847ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generated_data2[1000,:,0],label='generated data for GAN2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb94095",
   "metadata": {},
   "source": [
    "There is still the problem, that if you generate 1000 examples, they are always the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4372aa",
   "metadata": {},
   "source": [
    "# Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minmax scaler\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "df = pd.read_csv(\"./data/features.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# set index to date\n",
    "try:\n",
    "    df = df.set_index('Date').sort_index()\n",
    "except:\n",
    "    df = df\n",
    "\n",
    "# prep data\n",
    "df = df.drop(['Close-MA 20D', 'RSI 14D','Realized Volatility 30D','VIX Move'], axis=1)\n",
    "data = prep_data(df.values, 30, scaler)\n",
    "\n",
    "# check size\n",
    "print(len(data), data[0].shape)\n",
    "data = np.array(data)\n",
    "reshaped_data =  data.reshape(len(data),data.shape[1]*data.shape[2])\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abac12",
   "metadata": {},
   "source": [
    "## Visualizing synthetic data for GAN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Return\",\"Open-Close\",'Open-Low',\"Open-High\",\"Normalized Volume\", \"VIX\", \"VIX Open-close\"\n",
    "]\n",
    "\n",
    "# Plotting some generated samples. Both Synthetic and Original data are still standardized with values between [0, 1]\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "axes=axes.flatten()\n",
    "\n",
    "time = list(range(1,30))\n",
    "obs = np.random.randint(len(generated_data2.shape))\n",
    "unscaled_data = scaler.inverse_transform(data)\n",
    "unscaled_generated2 = scaler.inverse_transform(generated_data2)\n",
    "for j, col in enumerate(cols):\n",
    "    frame = pd.DataFrame({'Real': unscaled_data[obs][:, j],\n",
    "                   'Synthetic': unscaled_generated2[obs][:, j]})\n",
    "    frame.plot(ax=axes[j],\n",
    "            title = col,\n",
    "            secondary_y='Synthetic data', style=['-', '--'])\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574f64",
   "metadata": {},
   "source": [
    "## PCA for GAN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda44410",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "sample_size = 100\n",
    "# get random indexes\n",
    "idx = np.random.permutation(len(data))[:sample_size]\n",
    "real_sample = data[idx]\n",
    "synthetic_sample = generated_data2[idx]\n",
    "\n",
    "# reshape\n",
    "real_data_reduced = real_sample.reshape(-1, seq_len)\n",
    "synth_data_reduced = synthetic_sample.reshape(-1,seq_len)\n",
    "\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "pca.fit(real_data_reduced)\n",
    "\n",
    "pca_real = pd.DataFrame(pca.transform(real_data_reduced))\n",
    "pca_synth = pd.DataFrame(pca.transform(synth_data_reduced))\n",
    "\n",
    "data_reduced = np.concatenate((real_data_reduced, synth_data_reduced), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10, 5))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "\n",
    "ax = fig.add_subplot(spec[0,0])\n",
    "ax.set_title('PCA on baseline GAN')\n",
    "\n",
    "# PCA scatter plot\n",
    "plt.scatter(pca_real.iloc[:, 0].values, pca_real.iloc[:, 1].values,\n",
    "            c='black', alpha=0.2, label='Original')\n",
    "\n",
    "plt.scatter(pca_synth.iloc[:, 0], pca_synth.iloc[:, 1],\n",
    "            c='red', alpha=0.2, label='Synthetic')\n",
    "\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c95a7a",
   "metadata": {},
   "source": [
    "Better, but still what's happening is that if i generate 6000 samples, they are all exactly the same, which is why the pca is a single point. At least this time the points are neared to the true distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8149f",
   "metadata": {},
   "source": [
    "## TNSE for GAN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=n_components, n_iter=300)\n",
    "\n",
    "tsne_results = pd.DataFrame(tsne.fit_transform(data_reduced))\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(10, 5))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "\n",
    "ax2 = fig.add_subplot(spec[0,0])\n",
    "ax2.set_title('t-SNE on baseline GAN')\n",
    "\n",
    "# t-SNE scatter plot\n",
    "plt.scatter(tsne_results.iloc[:700, 0].values, tsne_results.iloc[:700, 1].values,\n",
    "            c='black', alpha=0.2, label='Original')\n",
    "plt.scatter(tsne_results.iloc[700:, 0], tsne_results.iloc[700:, 1],\n",
    "            c='red', alpha=0.2, label='Synthetic')\n",
    "\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data, f)\n",
    "with open(\"gen_data.pkl\", \"wb\") as f:\n",
    "    pkl.dump(generated_data2, f)\n",
    "\n",
    "print(generated_data2[0])\n",
    "print(generated_data2[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
